% !TEX root = ../main.tex

\begin{abstract}
互联网技术,多媒体技术以及数字成像技术的蓬勃发展导致了网络上图像内容数据的爆炸性增长，促使了图像检索技术的出现以及日渐成熟。
图像检索旨在从一个大规模的图像数据库中检索出和查询的图片对应的图像数据，是计算机视觉和多媒体检索领域一个核心的研究问题，因为其具有很高的科研价值以及广泛的工业界的应用，近些年在研究社区中获得了广泛的关注。
一个典型的图像检索系统包含两个阶段: (1) 表征学习: 学习一个算法提取图片物体的特征，将高维的图片数据映射到低维的特征向量空间。 (2) 度量学习: 使得第一步中学习到的特征向量具备高维图像数据的语义一致性。语义相似的图片的特征向量距离尽量小，相反，语义不同的的图像的特征向量距离尽量大。传统的图像检索使用实值的特征向量来表征图片, 虽然可以取得较高的准确度，但是由于内存占用量高以及检索速度不高, 导致其可扩展性差, 难以应用在对检索速度要求较高的大规模的图像检索场景中。 \par
深度哈希是一种利用深度神经网络来进行特征提取，将高维图片数据映射到低维二进制向量空间的技术，因为其具有内存占用量低，并且检索速度快等优点而被广泛应用在图像检索领域来提高图像检索系统的可扩展性。然而, 当深度哈希算法应用在具体的大规模图像检索应用上时还面临诸多具体的挑战。
本文从大规模图像检索的需求出发, 针对现有的基于深度哈希的算法在细粒度跨模态检索, 以及在类间差异小的应用场景下和通用检索中的表征学习以及度量学习中的不足, 提出了基于神经网络架构以及优化算法上的创新解决方案。 本文的主要创新点以及工作如下所示:
\begin{enumerate}
  \item 针对现有的算法在跨模态细粒度行人重识别图像检索中的提取出可辨别性特征能力的不足, 本文提出了从神经网络架构和度量学习算法两个方面协同创新的检索架构。传统的行人重识别问题针对单模态的数据,也就是检索的图片和数据库中的图片来自同一模态。而现实中的监控摄像头一般能同时在日间和夜间工作捕捉不同模态的行人照片,这使得跨模态行人重识别成为计算机视觉领域一个热门的研究课题。由于模态鸿沟的存在（Modality Gap）, 跨模态行人重识别比单模态行人重识别更具有挑战性。传统基于双流卷积神经网络的算法,很难学习到模态恒定（Modality-invariant）的特征。针对这一难点,本文创新性的基于自编码器（Autoencoder）设计了一个神经网络架构学习模态恒定以及外表恒定的特征。同时设计了适用于跨模态检索的用于对其不同模态特征的损失函数,使得网络可以在动态创建的图片对上进行监督训练。我们在标准的跨模态重识别数据集上进行了训练以及测试,结果表明我们的算法可以取得优越的性能数据,证明了算法的有效性。
  \item 针对现有的车辆图像检索的算法存储效率低以及检索速度慢以及现有的深度哈希算法在类间差异较小的场景下表征学习的难点, 本文首次提出一个基于困难三元组的的离散哈希架构, 探索了深度哈希在大规模车辆检索中的应用。本文提出一个新型的离散哈希模块,通过同时使用传统的基于困难三元组的损失函数进行特征学习,以及离散哈希模块生成离散的汉明哈希码。为了优化离散的哈希架构,我们提出了一个交替优化算法来进行整个架构的优化。 本文在主流的车辆重识别数据集上进行了四种不同长度哈希码的精度测试,显著的超越了当前的普适的哈希算法。
  \item 为了进一步优化深度哈希算法的表征能力,我们首个探索了不基于卷积神经网络的深度哈希框架。视觉 Transformer（Vision Transformer）是一种基于自注意力机制（self-attention）的新型的计算机视觉基础网络模型。本文提出了第一个完全基于视觉 Transformer的深度哈希框架,通过设计一个孪生视觉Transformer架构,以及一个新型的双流特征学习的视觉Transformer模块来进行细粒度表征学习。 同时,我们采取成对的基于贝叶斯的学习框架进行度量学习。本文在三个标准化的图像检索基准数据集上进行测试,实验效果表明该方法可以大幅度提升检索的性能。
  \item 由于基于传统哈希编码的方法会带来较大的精度损失的弱点, 本文探索一种基于乘积量化（Product Quantization）编码的深度哈希算法来提高检索性能。同时,本文提出第一个基于视觉Transformer的乘积量化神经网络。为了进行细粒度的特征学习,考虑到视觉Transformer的性能与采取的图片分块策略紧耦合的特点,本文设计了一个双支视觉Transformer量化的支柱网络。同时,本文设计了一个基于排序损失的直接优化平均查准率 （Average Precision）的量化损失函数来进行度量学习。文章将乘积量化集成到端到端的神经网络优化中, 取得了优异的性能表现。
\end{enumerate}




\end{abstract}

\begin{abstract*}
The remarkable advancements in internet, multimedia  and digital imaging technologies have prompted the explosive growth of multimedia data on the internet, which has spurred the advent and the rapid development of image retrieval technologies. Image retrieval is a key research topic in computer vision and multimedia retrieval which targets at retrieving the corresponding image in a large image database given a specific query image. Substantial attention has been drawn to the research of image retrieval in the community given its notable research value and its widespread application in the industry. A common learning paradigm of image retrieval typically incoporates two phases. (1) representation learning: it aims to learn discriminative feature representation from the image data, mapping the high-dimensional image data into low-dimensional feature vector space.  (2) metric learning: it targets at preserving the semantic consistency of the images in the low-dimensional feature space. The projected feature vectors of the semantically-similar images should be pulled as close as possible while those of  dissimilar images be pushed far away. Conventional image retrieval methods generally adopt real-valued feature vectors which enjoy the merit of being sufficently accurate. Nontheless, their high storage costs and low retrieval speed have limited their applicability in a large-scale image retrieval setting where the retrieval speed is of critical importance. \par
Deep hashing, which transforms high-dimensional image data into low-dimensional compact binary codes with deep neural networks, is a pervasive technology in industry.  It has been widespredly applied in image retrieval to promote the scalability of existing retrieval systems owing to its low memory usage and fast retrieval speed. Nontheless, there are still considerable challenges to be addressed when it is applied to specific large-scale image retrieval applications.
\begin{enumerate}
  \item Targeting at tackling the challenge of extracting find-grained discriminative features in a cross-modality person re-identification setting, we propose a novel deep neural network architecture in tandem with a retrofitted metric learning algorithm.   Conventional person re-identification focus on visible images captured by single-modality survelliance cameras. However, these visible light cameras fail to produce high-quality images under poor illumination conditions. Nowadays, the majority of the survelliance cameras are capable of automatically switching from visible to the infrared mode when the illumination condition is poor. This stimulates the researches on visible-infrared cross-modality person re-identification. The existence of the modality gap makes cross-modality person re-identification even more challenging. Previous methods with dual-stream convolutional neural networks generally fail to capture the modality-invariant features. To this end, we devise a novel modality and apperance invariant embedding learning framework for discriminative feature learning, which is equipped with retrofitted loss function for intra-modality and cross-modality retrieval. We conduct extensive experiments on standardized benchmark datasets which have evidenced the superiority of our proposed method.
  \item Targeting at addressing the low storage efficiency and slow retrieval speed of current vehicle retrieval methods and improving the representation learning of deep hashing algorithms, we propose a discrete hashing architecture based on hard triplets. Conventional methods with real-valued feature vectors generally consume tremendous memory and computation, making them inapplicable in a real-world large-scale retrieval scenario. To enhance the retrieval speed and optimize the storage cost, we propose a deep hashing based vehicle re-identification framework.  Specifically, we propose a novel discrete hashing module, which could produce discrete hashing codes. In addition, we adopt a traditional feature learning module with hard-mining triplet loss to perform similarity-preserving learning. To optimize the overall architecture, we propose an alternating optimization method for similarity-preserving hashing codes learning.  Comprehensice experiments on standard vehicle re-identification datasets have demonstrated our effectiveness and efficiency in terms of four hashing code lengths.
  \item To further enhance the representation ability of current deep hashing algorithms, we explore the possibilty of devising a deep hashing framework without convolutional neural networks. Vision transformer is variant of transformer tailored for computer vision tasks, which is based on the self-attention mechanism. We introduce the first deep hashing framework based on pure vision transformer. We design a siamese vision transformer backbone for feature extraction and innovate a novel dual-stream feature learning block to learn discriminative global and local features. In addition, we adopt a bayesian learning scheme with a dynamic constructed similarity matrix for similarity-preserving learning. We performed experiments on three benchmark datasets and the results have evidenced our superiority against the state-of-the-art deep hashing methods.
  \item Traditional hashing encoding scheme will result in considerable performance declines. To mitigate this problem,  we investigate a deep hashing algorithm based on product quantization to boost the retrieval performance. We propose the fist product quantization network based on pure vision transformer. Motivated by the fact that the performance of vision transformer is closely-related with the patching strategy, we put forward the a dual-brnach vision transformer-based quantization network. In addition, we innovate a novel quantization loss, dubbed average precision quantization loss, which embeds the asymmetric retrieval nature in product quantization-based methods into the metric learning process. The experiments on widely-studied benchmarks have demonstrated the effectiveness and superiority of our proposed quantization-based framework.
\end{enumerate}
\end{abstract*} 

